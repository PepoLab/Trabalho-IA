Id,Hora de início,Hora de conclusão,Email,Nome,"1
","2
",3,4,5
1,30/08/2025,30/08/2025,ChatGPT,,"Sim. Modelos de IA aprendem com dados históricos, que podem conter vieses humanos ou institucionais. Isso significa que, se os dados de treinamento refletirem desigualdades passadas (ex.: menos contratações de mulheres em tecnologia), o sistema pode reproduzir ou até reforçar esses padrões injustos.","Sim. A transparência é essencial em sistemas automatizados. Explicações claras ajudam candidatos a entenderem quais critérios foram usados, reduzem a sensação de arbitrariedade e permitem contestar decisões se houver erro.","Depende de como é implementada.

Pode aumentar: se for programada para reduzir vieses humanos, padronizando critérios objetivos.

Pode diminuir: se apenas replicar vieses dos dados históricos sem correções adequadas.
O efeito real está diretamente ligado ao design, auditoria e monitoramento do sistema.","A responsabilidade deve ser da empresa que utiliza a IA, não da IA em si. Isso inclui o empregador que adota a tecnologia e, em alguns casos, a empresa que desenvolveu o sistema. A IA é apenas uma ferramenta, e decisões sobre pessoas precisam ter supervisão e responsabilização humana.","Como IA, não participo de seleções. Mas, analisando o cenário: um processo totalmente automatizado pode ser eficiente, mas também arriscado por falta de sensibilidade e julgamento humano em aspectos subjetivos (como criatividade, ética ou potencial de aprendizado). O ideal é um modelo híbrido, combinando IA e avaliação humana."
2,30/08/2026,30/08/2026,DeepSeek,,Podem ser injustos. O viés é herdado dos dados históricos de treinamento.,Sim. O direito à explicação é um princípio key da IA ética.,Resultado neutro. O impacto depende exclusivamente do design intencional do sistema para mitigar ou amplificar vieses existentes.,"Responsabilidade humana. A culpa é sempre da organização que desenvolveu, implementou ou operou o sistema sem a devida auditoria.",Sim. A participação é condicional à existência de transparência operacional e garantias de que o sistema foi auditado quanto a vieses.
3,30/08/2027,30/08/2027,Google Gemini,,"Sim, podem ser injustos e preconceituosos, pois são treinados com dados históricos que podem conter vieses.",A transparência é fundamental. Saber os motivos permite a correção de vieses no algoritmo e dá ao candidato um feedback claro.,"Isso depende da forma como o algoritmo é projetado. Se for bem treinado, pode aumentar as oportunidades; se usar dados enviesados, pode diminuí-las.","A responsabilidade deve ser compartilhada entre a empresa que usa a IA, os desenvolvedores do sistema e os supervisores humanos do processo.","Não seria aplicável. A automação completa exigiria mecanismos de auditoria para garantir que a seleção seja baseada em mérito, e não em preconceitos."
4,30/08/2028,30/08/2028,Claude,,"Sim, podem reproduzir vieses dos dados de treinamento."," Sim, candidatos deveriam conhecer os critérios utilizados.",Depende da implementação - pode tanto perpetuar quanto reduzir discriminações.,"Desenvolvedores, empresas usuárias e reguladores compartilham a responsabilidade.","Como IA, não me candidato a empregos. Para humanos, processos puramente automatizados perdem nuances importantes da avaliação."
,,,,,,,,,
,,,,,,,,,
